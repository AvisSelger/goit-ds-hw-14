{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = 20000  # Size of the dictionary\n",
    "maxlen = 200  # Maximum length of sequence\n",
    "embedding_dim = 128  # The dimensionality of embedded word vectors\n",
    "lstm_units = 64  # Number of units in the LSTM layer\n",
    "dropout_rate = 0.5  # Percentage of neurons to cut\n",
    "batch_size = 64  # Battle size\n",
    "epochs = 5  # The number of learning epochs\n",
    "\n",
    "# Loading and preparing data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Building a model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
    "    LSTM(lstm_units, return_sequences=False),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "# Model assessment\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "\n",
    "model_rnn = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
    "    SimpleRNN(units=64, activation='tanh'),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
    "    LSTM(units=64, activation='tanh'),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "model_bi_lstm = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
    "    Bidirectional(LSTM(units=64, activation='tanh')),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_bi_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_lstm = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n",
    "    LSTM(units=64, return_sequences=True),\n",
    "    LSTM(units=64, return_sequences=False),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_deep_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Conclusions\n",
    "  - Application of Different RNN Types: The choice of RNN type depends on the\n",
    "task. Basic RNNs are often inadequate for complex tasks that require\n",
    "preserving long-term dependencies. LSTM and GRU are better choices as they\n",
    "handle long-term dependencies more effectively.\n",
    "  - Bidirectional Models: Bidirectional architectures are particularly useful when\n",
    "context from both directions is crucial for understanding the sequence. This\n",
    "can significantly improve accuracy in tasks where understanding the overall\n",
    "structure of the text is critical.\n",
    "  - Deep Architectures: Deep models can be more effective because they can\n",
    "learn more complex and abstract patterns. However, they are also more\n",
    "resource-intensive and more prone to overfitting, especially on small datasets.\n",
    "  - Regularization and Data Processing: It's important to use regularization\n",
    "techniques (e.g., Dropout, L2 regularization) and data augmentation to  \n",
    "prevent overfitting and improve the model's generalization capabilities.\n",
    "\n",
    "  Practical results may vary depending on the specific dataset, hyperparameter\n",
    "settings, and computational resources available for training the models.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
